<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/test/"/>
    <url>/test/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop集群搭建教程</title>
    <link href="/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <url>/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="一、环境安装"><a href="#一、环境安装" class="headerlink" title="一、环境安装"></a>一、环境安装</h3><h4 id="1-准备虚拟机"><a href="#1-准备虚拟机" class="headerlink" title="1. 准备虚拟机"></a>1. 准备虚拟机</h4><p>使用VMware或hyper-v创建三台虚拟机（系统为Ubuntu18.04）。</p><h4 id="2-设置静态IP、修改主机名和hosts"><a href="#2-设置静态IP、修改主机名和hosts" class="headerlink" title="2. 设置静态IP、修改主机名和hosts"></a>2. 设置静态IP、修改主机名和hosts</h4><ul><li><p>设置静态IP</p><p>修改目录/etc/netplan下的配置文件01-netcfg.yaml（文件名可能不一样）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop:~# vim /etc/netplan/01-netcfg.yaml<br></code></pre></td></tr></table></figure><p>将dhcp4更改为no，并设置IP地址、网关和DNS服务器，配置如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> This file describes the network interfaces available on your system</span><br><span class="hljs-meta">#</span><span class="bash"> For more information, see netplan(5).</span><br>network:<br>  version: 2<br>  renderer: networkd<br>  ethernets:<br>    eth0:<br>      dhcp4: no<br>      addresses: [172.22.160.101/20]    #IP地址和掩码<br>      gateway4: 172.22.160.1            #网关<br>      nameservers:                      #DNS<br>              addresses: [114.114.114.114]<br><br></code></pre></td></tr></table></figure><p>另外两台主机的IP分别为：172.22.160.102、172.22.160.103</p><p><strong>注意：</strong>IP设置成自己的，确保三台主机在同一局域网下，且能够相互ping通</p></li></ul><ul><li><p>修改主机名</p><p>将3台主机名修改成不同的名称，分别为Hadoop101、Hadoop102、Hadoop103，便于区分。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/hostname<br></code></pre></td></tr></table></figure></li></ul><ul><li><p>修改hosts文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/hosts<br></code></pre></td></tr></table></figure><p>添加以下三条记录（IP需要修改成自己的）</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">172.22.160.101</span>Hadoop101<br><span class="hljs-number">172.22.160.102</span>Hadoop102<br><span class="hljs-number">172.22.160.103</span>Hadoop103<br></code></pre></td></tr></table></figure><h4 id="3-配置ssh"><a href="#3-配置ssh" class="headerlink" title="3. 配置ssh"></a>3. 配置ssh</h4></li></ul><h5 id="1-安装ssh"><a href="#1-安装ssh" class="headerlink" title="1. 安装ssh"></a>1. 安装ssh</h5><p>检查是否安装了ssh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# ps -e | grep ssh<br> 1048 ?        00:00:00 sshd<br> 1097 ?        00:00:00 sshd<br> 1217 ?        00:00:00 sshd<br></code></pre></td></tr></table></figure><p>若没有出现上述信息，则需要安装ssh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt-get install openssh-server<br></code></pre></td></tr></table></figure><h5 id="2-允许root通过ssh登录"><a href="#2-允许root通过ssh登录" class="headerlink" title="2. 允许root通过ssh登录"></a>2. 允许root通过ssh登录</h5><p>修改sshd_config文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hadoop@Hadoop102:~$ sudo vim /etc/ssh/sshd_config<br></code></pre></td></tr></table></figure><p>找到PermitRootLogin，修改为如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">LoginGraceTime 2m</span><br><span class="hljs-meta">#</span><span class="bash">PermitRootLogin prohibit-password</span><br>PermitRootLogin yes<br><span class="hljs-meta">#</span><span class="bash">StrictModes yes</span><br><span class="hljs-meta">#</span><span class="bash">MaxAuthTries 6</span><br><span class="hljs-meta">#</span><span class="bash">MaxSessions 10</span><br></code></pre></td></tr></table></figure><h5 id="3-设置免密登录"><a href="#3-设置免密登录" class="headerlink" title="3. 设置免密登录"></a>3. 设置免密登录</h5><p>在主机Hadoop101上生成密钥公钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# ssh-keygen -t rsa<br></code></pre></td></tr></table></figure><p>将公钥发送到主机Hadoop102和Hadoop103</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# ssh-copy-id hadoop102<br>root@Hadoop101:~# ssh-copy-id hadoop103<br></code></pre></td></tr></table></figure><h4 id="4-编写分发脚本"><a href="#4-编写分发脚本" class="headerlink" title="4. 编写分发脚本"></a>4. 编写分发脚本</h4><p>一般情况下，服务器的数量比较多，在每台服务器上分别配置Hadoop不太现实，rsync可以将文件同步到其他服务器，以下脚本可以群发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><br><span class="hljs-meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span><br>pcount=$#<br>if((pcount==0)); then<br>echo no args;<br>exit;<br>fi<br><br><span class="hljs-meta">#</span><span class="bash">2 获取文件名称</span><br>p1=$1<br>fname=`basename $p1`<br>echo fname=$fname<br><br><span class="hljs-meta">#</span><span class="bash">3 获取上级目录到绝对路径</span><br>pdir=`cd -P $(dirname $p1); pwd`<br>echo pdir=$pdir<br><br><span class="hljs-meta">#</span><span class="bash">4 获取当前用户名称</span><br>user=`whoami`<br><br><span class="hljs-meta">#</span><span class="bash">5 循环，这里host根据自己的节点数和主机名设置</span><br>for((host=102; host&lt;104; host++)); do<br>        #echo $pdir/$fname $user@hadoop$host:$pdir<br>        echo --------------- hadoop$host ----------------<br>        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir<br>done<br></code></pre></td></tr></table></figure><p>将脚本放在/usr/sbin目录下，并赋予777权限</p><h4 id="5-安装jre和Hadoop"><a href="#5-安装jre和Hadoop" class="headerlink" title="5. 安装jre和Hadoop"></a>5. 安装jre和Hadoop</h4><p>解压server-jre-8u261-linux-x64.tar.gz到/usr/java</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# mkdir /usr/java<br>root@Hadoop101:~# tar -zxvf server-jre-8u261-linux-x64.tar.gz<br>root@Hadoop101:~# mv jdk1.8.0_261/ /usr/java/<br></code></pre></td></tr></table></figure><p>解压hadoop-3.2.1.tar.gz到/usr/hadoop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# mkdir /usr/hadoop<br>root@Hadoop101:~# tar -zxvf hadoop-3.2.1.tar.gz<br>root@Hadoop101:~# mv hadoop-3.2.1 /usr/hadoop/<br></code></pre></td></tr></table></figure><p>添加环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/profile<br></code></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">export JAVA_HOME=/usr/java/jdk1.8.0_261<br>export HADOOP_HOME=/usr/hadoop/hadoop-3.2.1<br>export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin<br></code></pre></td></tr></table></figure><h3 id="二、配置Hadoop集群"><a href="#二、配置Hadoop集群" class="headerlink" title="二、配置Hadoop集群"></a>二、配置Hadoop集群</h3><p>切换到目录/usr/hadoop/hadoop-3.2.1/etc/hadoop/，修改配置文件</p><h4 id="1-hadoop-env-sh"><a href="#1-hadoop-env-sh" class="headerlink" title="1. hadoop-env.sh"></a>1. hadoop-env.sh</h4><p>在该文件中添加JAVA_HOME</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> The java implementation to use. By default, this environment</span><br><span class="hljs-meta">#</span><span class="bash"> variable is REQUIRED on ALL platforms except OS X!</span><br>export JAVA_HOME=/usr/java/jdk1.8.0_261<br></code></pre></td></tr></table></figure><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2. core-site.xml"></a>2. core-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/usr/hadoop/hadoop-3.2.1/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br></code></pre></td></tr></table></figure><h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3. hdfs-site.xml"></a>3. hdfs-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- hdfs存储数据的副本数量（避免一台宕机），可以不设置，默认值是3 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <br><span class="hljs-comment">&lt;!--hdfs 监听namenode的web的地址，默认就是9870端口，如果不改端口也可以不设置 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop101:9870<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <br><span class="hljs-comment">&lt;!-- hdfs保存datanode当前数据的路径，默认值需要配环境变量，建议使用自己创建的路径，方便管理--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/usr/hadoop/hadoop-3.2.1/hdfs/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <br><span class="hljs-comment">&lt;!-- hdfs保存namenode当前数据的路径，默认值需要配环境变量，建议使用自己创建的路径，方便管理--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/usr/hadoop/hadoop-3.2.1/hdfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <br></code></pre></td></tr></table></figure><h4 id="4-yarn-site-xml"><a href="#4-yarn-site-xml" class="headerlink" title="4. yarn-site.xml"></a>4. yarn-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 必须配置指定YARN的老大（ResourceManager）在哪一台主机 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop101<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <br><span class="hljs-comment">&lt;!-- 必须配置提供mapreduce程序获取数据的方式 默认为空 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="5-mapred-site-xml"><a href="#5-mapred-site-xml" class="headerlink" title="5. mapred-site.xml"></a>5. mapred-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 必须设置，mapreduce程序使用的资源调度平台，默认值是local，若不改就只能单机运行，不会到集群上了 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-comment">&lt;!-- 这是3.2以上版本需要增加配置的，不配置运行mapreduce任务可能会有问题，记得使用自己的路径 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.application.classpath<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span><br>        /usr/hadoop/hadoop-3.2.1/etc/hadoop,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/common/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/common/lib/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/hdfs/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/hdfs/lib/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/mapreduce/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/mapreduce/lib/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/yarn/*,<br>        /usr/hadoop/hadoop-3.2.1/share/hadoop/yarn/lib/*<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="6-配置workers"><a href="#6-配置workers" class="headerlink" title="6. 配置workers"></a>6. 配置workers</h4><p>配置需要启动datanode节点的主机</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xml">hadoop101<br>hadoop102<br>hadoop103<br></code></pre></td></tr></table></figure><h4 id="7-分发配置文件"><a href="#7-分发配置文件" class="headerlink" title="7. 分发配置文件"></a>7. 分发配置文件</h4><p>将配置好的文件分发到主机hadoop102和hadoop103上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">xsync /usr/hadoop/hadoop-3.2.1/etc/hadoop/<br></code></pre></td></tr></table></figure><h4 id="8-启动集群"><a href="#8-启动集群" class="headerlink" title="8. 启动集群"></a>8. 启动集群</h4><p>在/hadoop/sbin路径下，将以下参数添加到start-dfs.sh和stop-dfs.sh的顶部</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">HDFS_DATANODE_USER=root<br>HADOOP_SECURE_DN_USER=hdfs<br>HDFS_NAMENODE_USER=root<br>HDFS_SECONDARYNAMENODE_USER=root<br></code></pre></td></tr></table></figure><p>将以下参数添加到start-yarn.sh和stop-yarn.sh的顶部</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">YARN_RESOURCEMANAGER_USER=root<br>HADOOP_SECURE_DN_USER=yarn<br>YARN_NODEMANAGER_USER=root<br></code></pre></td></tr></table></figure><p>启动hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@Hadoop101:~# start-dfs.sh<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>其它</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
